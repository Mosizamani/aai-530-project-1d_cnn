{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WESAD Data Cleaning & Exploratory Data Analysis\n",
    "\n",
    "This notebook combines:\n",
    "- **Part 1 – Data Cleaning**: WESAD chest sensor data (ACC, ECG, EDA, EMG, Resp, Temp, events, questionnaires)\n",
    "- **Part 2 – EDA**: Aligned wrist BVP/EDA data at 64 Hz with stress labels (0=baseline, 1=stress, 2=amusement, -1=other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Data Cleaning (WESAD Chest Data)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "merged_dir = '/Users/mostafazamaniturk/Documents/USD/AAI-530/work/WESAD_data/merged_cleaned_data'\n",
    "\n",
    "# Large files (~55M rows); sample to avoid memory issues\n",
    "large_files = {'merged_chest_acc.csv', 'merged_wrist_acc.csv'}\n",
    "large_nrows = 100_000\n",
    "\n",
    "csv_files = [\n",
    "    'merged_chest_acc.csv',\n",
    "    'merged_chest_ecg.csv',\n",
    "    'merged_chest_eda.csv',\n",
    "    'merged_chest_emg.csv',\n",
    "    'merged_chest_resp.csv',\n",
    "    'merged_chest_temp.csv',\n",
    "    'merged_event_timings.csv',\n",
    "    'merged_questionnaire_responses.csv',\n",
    "    'merged_wrist_acc.csv',\n",
    "    'merged_wrist_bvp.csv',\n",
    "    'merged_wrist_eda.csv',\n",
    "    'merged_wrist_temp.csv',\n",
    "]\n",
    "\n",
    "for fname in csv_files:\n",
    "    path = os.path.join(merged_dir, fname)\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"File not found: {fname}\")\n",
    "        continue\n",
    "\n",
    "    nrows = large_nrows if fname in large_files else None\n",
    "    df = pd.read_csv(path, nrows=nrows)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"FILE: {fname}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n.head():\\n\", df.head())\n",
    "    print(\"\\n.tail():\\n\", df.tail())\n",
    "    print(\"\\n.describe():\\n\", df.describe(include='all'))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KtEx43QTbEJ2",
    "outputId": "5897603d-774f-4983-cd7a-6e4d24d1190d"
   },
   "outputs": [],
   "source": [
    "! pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66158b95",
    "outputId": "e1d11ead-e54e-46bc-e605-c312eb8f7119"
   },
   "outputs": [],
   "source": [
    "print('Listing contents of MyDrive:')\n",
    "! ls \"/Users/mostafazamaniturk/Documents/USD/AAI-530/work\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0afa312a"
   },
   "source": [
    "If your `WESAD.zip` file is inside a subfolder, for example, named `MyProject`, you would then run:\n",
    "\n",
    "```python\n",
    "!ls \"/Users/mostafazamaniturk/Documents/USD/AAI-530/work\"\n",
    "```\n",
    "\n",
    "Once you find the file, use the full path in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yH0pkVivmCPM",
    "outputId": "9eb8f2f0-4226-45f0-8f3b-92503ca971e3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Define paths using your local structure\n",
    "base_path = r'/Users/mostafazamaniturk/Documents/USD/AAI-530/work'\n",
    "zip_path = os.path.join(base_path, 'WESAD.zip')\n",
    "extract_path = os.path.join(base_path, 'WESAD_data')\n",
    "\n",
    "if os.path.exists(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(f\"Extraction complete! Files are in: {extract_path}\")\n",
    "else:\n",
    "    print(\"Zip file not found. Check the path again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aa8abf94"
   },
   "source": [
    "# Task\n",
    "Explore the structure of the unzipped WESAD data by listing the contents of the `/content/WESAD_data` directory to identify the main data files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9a124f70"
   },
   "source": [
    "## Explore Unzipped Data\n",
    "\n",
    "### Subtask:\n",
    "Explore the structure of the unzipped WESAD data by listing the contents of the `/content/WESAD_data` directory to identify the main data files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a59b22cf"
   },
   "source": [
    "**Reasoning**:\n",
    "To explore the structure of the unzipped WESAD data, I will list the contents of the `/content/WESAD_data` directory using the `!ls -F` command as instructed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ebd61aa7",
    "outputId": "10ee29c5-23be-44cc-8bf2-b3637b3b991d"
   },
   "outputs": [],
   "source": [
    "print('Listing contents of the unzipped WESAD data directory:')\n",
    "!ls -F /Users/mostafazamaniturk/Documents/USD/AAI-530/work/WESAD_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba54179c"
   },
   "source": [
    "**Reasoning**:\n",
    "Since the previous `ls` command revealed a subdirectory named `WESAD/`, I will now list the contents of this subdirectory to further explore the data structure and locate the actual data files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbb4aa9e",
    "outputId": "c65d9e39-9606-4e4e-d1c1-0a5feb56c731"
   },
   "outputs": [],
   "source": [
    "print('Listing contents of the WESAD subdirectory:')\n",
    "!ls -F /Users/mostafazamaniturk/Documents/USD/AAI-530/work/WESAD_data/WESAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6a5b1e7"
   },
   "source": [
    "**Reasoning**:\n",
    "The previous command revealed subject-specific subdirectories (S2/, S3/, etc.) and a `wesad_readme.pdf` file. To understand the data structure further, I will list the contents of one of these subject directories, specifically `S2/`, to see what kind of data files are stored there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "635b6015",
    "outputId": "a15f11e7-edbd-48fd-bd79-4a51da8e83ad"
   },
   "outputs": [],
   "source": [
    "print('Listing contents of the S2 subdirectory:')\n",
    "!ls -F /Users/mostafazamaniturk/Documents/USD/AAI-530/work/WESAD_data/WESAD/S2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e963a801"
   },
   "source": [
    "## Load Data\n",
    "\n",
    "### Subtask:\n",
    "Load the relevant WESAD data files into pandas DataFrames for cleaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c05627fe",
    "outputId": "95e9caf8-007f-4a29-d329-35b0ee6a599a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the base path to the S2 subject data directory\n",
    "s2_data_path = '/Users/mostafazamaniturk/Documents/USD/AAI-530/work/WESAD_data/WESAD/S2'\n",
    "\n",
    "# Load the S2.pkl file into a dictionary first\n",
    "s2_raw_data_dict = pd.read_pickle(f'{s2_data_path}/S2.pkl')\n",
    "\n",
    "# Initialize dictionaries to hold individual sensor DataFrames\n",
    "chest_dataframes = {}\n",
    "wrist_dataframes = {}\n",
    "\n",
    "# Process 'chest' signals\n",
    "chest_signals = s2_raw_data_dict['signal']['chest']\n",
    "for sensor_name, data_array in chest_signals.items():\n",
    "    if sensor_name == 'ACC':\n",
    "        # Accelerometer data is 3-axis, create three columns\n",
    "        chest_dataframes[sensor_name] = pd.DataFrame(data_array, columns=['ACC_x', 'ACC_y', 'ACC_z'])\n",
    "    else:\n",
    "        # Other sensors are typically 1D or 2D with one column, flatten and create a DataFrame\n",
    "        chest_dataframes[sensor_name] = pd.DataFrame(data_array.flatten(), columns=[sensor_name])\n",
    "\n",
    "# Process 'wrist' signals\n",
    "wrist_signals = s2_raw_data_dict['signal']['wrist']\n",
    "for sensor_name, data_array in wrist_signals.items():\n",
    "    if sensor_name == 'ACC':\n",
    "        # Accelerometer data is 3-axis, create three columns\n",
    "        wrist_dataframes[sensor_name] = pd.DataFrame(data_array, columns=['ACC_x', 'ACC_y', 'ACC_z'])\n",
    "    else:\n",
    "        # Other sensors are typically 1D or 2D with one column, flatten and create a DataFrame\n",
    "        wrist_dataframes[sensor_name] = pd.DataFrame(data_array.flatten(), columns=[sensor_name])\n",
    "\n",
    "# Load the S2_quest.csv file into a pandas DataFrame\n",
    "df_s2_quest = pd.read_csv(f'{s2_data_path}/S2_quest.csv')\n",
    "\n",
    "print(\"Chest DataFrames (first 5 rows of each):\")\n",
    "for sensor_name, df in chest_dataframes.items():\n",
    "    print(f\"\\n{sensor_name}:\")\n",
    "    print(df.head())\n",
    "\n",
    "print(\"\\nWrist DataFrames (first 5 rows of each):\")\n",
    "for sensor_name, df in wrist_dataframes.items():\n",
    "    print(f\"\\n{sensor_name}:\")\n",
    "    print(df.head())\n",
    "\n",
    "print(\"\\ndf_s2_quest head:\")\n",
    "print(df_s2_quest.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0b99604"
   },
   "source": [
    "## Initial Data Inspection\n",
    "\n",
    "### Subtask:\n",
    "Perform an initial inspection of the loaded data, including checking the first few rows (.head()), data types (.info()), and descriptive statistics (.describe()) to identify potential issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7daef674"
   },
   "source": [
    "**Reasoning**:\n",
    "To perform an initial inspection, I will iterate through the `chest_dataframes` and `wrist_dataframes` dictionaries, applying `.head()`, `.info()`, and `.describe()` to each sensor DataFrame. I will also apply these methods to the `df_s2_quest` DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3687c834",
    "outputId": "c11d8c0c-5a40-4b4d-eafb-8b04fe87879a"
   },
   "outputs": [],
   "source": [
    "print(\"--- Initial Data Inspection for Chest Sensor DataFrames ---\")\n",
    "for sensor_name, df in chest_dataframes.items():\n",
    "    print(f\"\\n----- {sensor_name} Data (Chest) -----\")\n",
    "    print(f\"\\n{sensor_name}.head():\")\n",
    "    print(df.head())\n",
    "    print(f\"\\n{sensor_name}.info():\")\n",
    "    df.info()\n",
    "    print(f\"\\n{sensor_name}.describe():\")\n",
    "    print(df.describe())\n",
    "\n",
    "print(\"\\n--- Initial Data Inspection for Wrist Sensor DataFrames ---\")\n",
    "for sensor_name, df in wrist_dataframes.items():\n",
    "    print(f\"\\n----- {sensor_name} Data (Wrist) -----\")\n",
    "    print(f\"\\n{sensor_name}.head():\")\n",
    "    print(df.head())\n",
    "    print(f\"\\n{sensor_name}.info():\")\n",
    "    df.info()\n",
    "    print(f\"\\n{sensor_name}.describe():\")\n",
    "    print(df.describe())\n",
    "\n",
    "print(\"\\n--- Initial Data Inspection for Questionnaire Data (df_s2_quest) ---\")\n",
    "print(\"\\ndf_s2_quest.head():\")\n",
    "print(df_s2_quest.head())\n",
    "print(\"\\ndf_s2_quest.info():\")\n",
    "df_s2_quest.info()\n",
    "print(\"\\ndf_s2_quest.describe():\")\n",
    "print(df_s2_quest.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define paths - works for both local and Colab\n",
    "base_path = r'/Users/mostafazamaniturk/Documents/USD/AAI-530/work'\n",
    "wesad_base = os.path.join(base_path, 'WESAD_data', 'WESAD')\n",
    "\n",
    "# All subject folders to process\n",
    "SUBJECTS = ['S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10', 'S11', 'S13', 'S14', 'S15', 'S16', 'S17']\n",
    "\n",
    "def clean_split_list(s):\n",
    "    parts = s.replace('#', '').split(';')\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def process_subject(subject):\n",
    "    \"\"\"Load, clean, parse, and save data for one subject.\"\"\"\n",
    "    subject_path = os.path.join(wesad_base, subject)\n",
    "    pkl_path = os.path.join(subject_path, f'{subject}.pkl')\n",
    "    quest_path = os.path.join(subject_path, f'{subject}_quest.csv')\n",
    "    \n",
    "    if not os.path.exists(pkl_path) or not os.path.exists(quest_path):\n",
    "        print(f\"Skipping {subject}: missing pkl or quest file\")\n",
    "        return None\n",
    "    \n",
    "    # Load pkl\n",
    "    raw_data = pd.read_pickle(pkl_path)\n",
    "    chest_dataframes = {}\n",
    "    wrist_dataframes = {}\n",
    "    \n",
    "    # Process chest signals\n",
    "    for sensor_name, data_array in raw_data['signal']['chest'].items():\n",
    "        if sensor_name == 'ACC':\n",
    "            chest_dataframes[sensor_name] = pd.DataFrame(data_array, columns=['ACC_x', 'ACC_y', 'ACC_z'])\n",
    "        else:\n",
    "            chest_dataframes[sensor_name] = pd.DataFrame(data_array.flatten(), columns=[sensor_name])\n",
    "    \n",
    "    # Process wrist signals\n",
    "    for sensor_name, data_array in raw_data['signal']['wrist'].items():\n",
    "        if sensor_name == 'ACC':\n",
    "            wrist_dataframes[sensor_name] = pd.DataFrame(data_array, columns=['ACC_x', 'ACC_y', 'ACC_z'])\n",
    "        else:\n",
    "            wrist_dataframes[sensor_name] = pd.DataFrame(data_array.flatten(), columns=[sensor_name])\n",
    "    \n",
    "    # FULL SESSION: Do NOT truncate. Export entire recording (~1.5 hours per subject).\n",
    "    for sensor_name in chest_dataframes:\n",
    "        chest_dataframes[sensor_name] = chest_dataframes[sensor_name].reset_index(drop=True)\n",
    "    for sensor_name in wrist_dataframes:\n",
    "        wrist_dataframes[sensor_name] = wrist_dataframes[sensor_name].reset_index(drop=True)\n",
    "    # Sanity: expect ~90 min per subject (wrist BVP at 64 Hz)\n",
    "    if 'BVP' in wrist_dataframes:\n",
    "        dur_min = len(wrist_dataframes['BVP']) / 64.0 / 60.0\n",
    "        print(f\"  {subject}: {len(wrist_dataframes['BVP'])} BVP samples = {dur_min:.1f} min\")\n",
    "    \n",
    "    # Load and parse questionnaire\n",
    "    df_quest = pd.read_csv(quest_path)\n",
    "    order_str = df_quest.iloc[0, 0]\n",
    "    start_str = df_quest.iloc[1, 0]\n",
    "    end_str = df_quest.iloc[2, 0]\n",
    "    \n",
    "    event_names = clean_split_list(order_str)[1:]\n",
    "    start_times = {event: float(clean_split_list(start_str)[i+1]) if (i+1) < len(clean_split_list(start_str)) else None \n",
    "                   for i, event in enumerate(event_names)}\n",
    "    end_times = {event: float(clean_split_list(end_str)[i+1]) if (i+1) < len(clean_split_list(end_str)) else None \n",
    "                 for i, event in enumerate(event_names)}\n",
    "    \n",
    "    df_event_timings = pd.DataFrame({\n",
    "        'Event': event_names,\n",
    "        'Start_Time': [start_times.get(e) for e in event_names],\n",
    "        'End_Time': [end_times.get(e) for e in event_names]\n",
    "    })\n",
    "    df_event_timings['Duration'] = df_event_timings['End_Time'] - df_event_timings['Start_Time']\n",
    "    \n",
    "    # Parse questionnaire responses\n",
    "    panas_data, stai_data, dim_data, sssq_data = [], [], [], []\n",
    "    for i in range(4, len(df_quest)):\n",
    "        row_string = df_quest.iloc[i, 0]\n",
    "        if row_string.startswith('# PANAS'):\n",
    "            parts = row_string.replace('# PANAS;', '').split(';')\n",
    "            panas_data.append([float(p.strip()) for p in parts if p.strip()])\n",
    "        elif row_string.startswith('# STAI'):\n",
    "            parts = row_string.replace('# STAI;', '').split(';')\n",
    "            stai_data.append([float(p.strip()) for p in parts if p.strip()])\n",
    "        elif row_string.startswith('# DIM'):\n",
    "            parts = row_string.replace('# DIM;', '').split(';')\n",
    "            dim_data.append([float(p.strip()) for p in parts if p.strip()])\n",
    "        elif row_string.startswith('# SSSQ'):\n",
    "            parts = row_string.replace('# SSSQ;', '').split(';')\n",
    "            sssq_data.append([float(p.strip()) for p in parts if p.strip()])\n",
    "    \n",
    "    df_panas = pd.DataFrame(panas_data)\n",
    "    df_stai = pd.DataFrame(stai_data)\n",
    "    df_dim = pd.DataFrame(dim_data)\n",
    "    df_sssq = pd.DataFrame(sssq_data)\n",
    "    \n",
    "    df_panas.columns = [f'PANAS_{i}' for i in range(len(df_panas.columns))]\n",
    "    df_stai.columns = [f'STAI_{i}' for i in range(len(df_stai.columns))]\n",
    "    df_dim.columns = [f'DIM_{i}' for i in range(len(df_dim.columns))]\n",
    "    df_sssq.columns = [f'SSSQ_{i}' for i in range(len(df_sssq.columns))]\n",
    "    \n",
    "    df_combined = pd.concat([df_panas, df_stai, df_dim], axis=1)\n",
    "    df_sssq_ext = pd.DataFrame(np.nan, index=range(len(df_combined)), columns=df_sssq.columns)\n",
    "    if not df_sssq.empty:\n",
    "        df_sssq_ext.iloc[0] = df_sssq.iloc[0]\n",
    "    df_questionnaire_responses = pd.concat([df_combined, df_sssq_ext], axis=1)\n",
    "    \n",
    "    # Save to subject's cleaned_data folder\n",
    "    cleaned_data_dir = os.path.join(subject_path, 'cleaned_data')\n",
    "    os.makedirs(cleaned_data_dir, exist_ok=True)\n",
    "    \n",
    "    df_event_timings.to_csv(os.path.join(cleaned_data_dir, 'df_event_timings.csv'), index=False)\n",
    "    df_questionnaire_responses.to_csv(os.path.join(cleaned_data_dir, 'df_questionnaire_responses.csv'), index=False)\n",
    "    for sensor_name, df in chest_dataframes.items():\n",
    "        df.to_csv(os.path.join(cleaned_data_dir, f'chest_{sensor_name.lower()}.csv'), index=False)\n",
    "    for sensor_name, df in wrist_dataframes.items():\n",
    "        df.to_csv(os.path.join(cleaned_data_dir, f'wrist_{sensor_name.lower()}.csv'), index=False)\n",
    "    \n",
    "    return {\n",
    "        'subject': subject,\n",
    "        'df_event_timings': df_event_timings,\n",
    "        'df_questionnaire_responses': df_questionnaire_responses,\n",
    "        'chest_dataframes': chest_dataframes,\n",
    "        'wrist_dataframes': wrist_dataframes,\n",
    "    }\n",
    "\n",
    "# Process all subjects\n",
    "all_results = []\n",
    "for subject in SUBJECTS:\n",
    "    print(f\"Processing {subject}...\")\n",
    "    result = process_subject(subject)\n",
    "    if result is not None:\n",
    "        all_results.append(result)\n",
    "        print(f\"  Saved cleaned data for {subject}\")\n",
    "\n",
    "# Merge event timings and questionnaire responses\n",
    "merged_event_timings = []\n",
    "merged_questionnaire = []\n",
    "for r in all_results:\n",
    "    et = r['df_event_timings'].copy()\n",
    "    et['subject'] = r['subject']\n",
    "    merged_event_timings.append(et)\n",
    "    qr = r['df_questionnaire_responses'].copy()\n",
    "    qr['subject'] = r['subject']\n",
    "    merged_questionnaire.append(qr)\n",
    "\n",
    "df_merged_event_timings = pd.concat(merged_event_timings, ignore_index=True)\n",
    "df_merged_questionnaire = pd.concat(merged_questionnaire, ignore_index=True)\n",
    "\n",
    "# Merge sensor data (with subject column)\n",
    "# Map file names to actual dict keys (chest: Temp/Resp, wrist: TEMP)\n",
    "sensor_map = {\n",
    "    'chest_acc': ('chest', 'ACC'), 'chest_ecg': ('chest', 'ECG'), 'chest_emg': ('chest', 'EMG'),\n",
    "    'chest_eda': ('chest', 'EDA'), 'chest_temp': ('chest', 'Temp'), 'chest_resp': ('chest', 'Resp'),\n",
    "    'wrist_acc': ('wrist', 'ACC'), 'wrist_bvp': ('wrist', 'BVP'), 'wrist_eda': ('wrist', 'EDA'),\n",
    "    'wrist_temp': ('wrist', 'TEMP')\n",
    "}\n",
    "merged_sensors = {}\n",
    "for fname, (sensor_type, sensor_key) in sensor_map.items():\n",
    "    parts = []\n",
    "    for r in all_results:\n",
    "        dfs = r['chest_dataframes'] if sensor_type == 'chest' else r['wrist_dataframes']\n",
    "        if sensor_key in dfs:\n",
    "            df = dfs[sensor_key].copy()\n",
    "            df['subject'] = r['subject']\n",
    "            parts.append(df)\n",
    "    if parts:\n",
    "        merged_sensors[fname] = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "# Save merged dataset\n",
    "merged_dir = os.path.join(base_path, 'WESAD_data', 'merged_cleaned_data')\n",
    "os.makedirs(merged_dir, exist_ok=True)\n",
    "print(f\"\\nSaving merged dataset to: {merged_dir}\")\n",
    "\n",
    "df_merged_event_timings.to_csv(os.path.join(merged_dir, 'merged_event_timings.csv'), index=False)\n",
    "df_merged_questionnaire.to_csv(os.path.join(merged_dir, 'merged_questionnaire_responses.csv'), index=False)\n",
    "for fname, df in merged_sensors.items():\n",
    "    df.to_csv(os.path.join(merged_dir, f'merged_{fname}.csv'), index=False)\n",
    "\n",
    "print(f\"Saved merged_event_timings.csv ({len(df_merged_event_timings)} rows)\")\n",
    "print(f\"Saved merged_questionnaire_responses.csv ({len(df_merged_questionnaire)} rows)\")\n",
    "for fname, df in merged_sensors.items():\n",
    "    print(f\"Saved merged_{fname}.csv ({len(df)} rows)\")\n",
    "print(\"\\nAll cleaned data saved per subject and merged dataset saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full-Session Export Pipeline (REVISED)\n",
    "\n",
    "**Requirements:**\n",
    "1. **Load from original WESAD pickle files** – Export the entire session (~1.5 hours per subject), not truncated.\n",
    "2. **Use merged_event_timings.csv** – Map timestamps to true events from the questionnaire-derived timing file.\n",
    "3. **Label column** – Generate labels: 0=Baseline, 1=Stress (TSST), 2=Amusement (Fun), -1=Other/Transition (Medi, Read).\n",
    "\n",
    "Event times in `merged_event_timings.csv` (from `*_quest.csv`) are in **minutes** from session start. We convert to seconds when mapping to sensor `time_sec`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, We are going to release information from merged datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "merged_dir = '/Users/mostafazamaniturk/Documents/USD/AAI-530/work/WESAD_data/merged_cleaned_data'\n",
    "\n",
    "# 1. File inventory and sizes\n",
    "print(\"=\" * 60)\n",
    "print(\"MERGED CSV FILES - INVENTORY\")\n",
    "print(\"=\" * 60)\n",
    "csv_files = sorted([f for f in os.listdir(merged_dir) if f.endswith('.csv')])\n",
    "for f in csv_files:\n",
    "    path = os.path.join(merged_dir, f)\n",
    "    size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "    print(f\"  {f}: {size_mb:.2f} MB\")\n",
    "\n",
    "# 2. Load and extract info from event timings\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVENT TIMINGS\")\n",
    "print(\"=\" * 60)\n",
    "df_events = pd.read_csv(os.path.join(merged_dir, 'merged_event_timings.csv'))\n",
    "print(f\"Total rows: {len(df_events)}\")\n",
    "print(f\"Unique subjects: {df_events['subject'].nunique()}\")\n",
    "print(f\"Unique events: {df_events['Event'].unique().tolist()}\")\n",
    "df_events['Duration'] = df_events['End_Time'] - df_events['Start_Time']\n",
    "print(\"\\nDuration (minutes) per event type:\")\n",
    "print(df_events.groupby('Event')['Duration'].agg(['mean', 'min', 'max']).round(2))\n",
    "print(\"\\nSample:\")\n",
    "print(df_events.head(10))\n",
    "\n",
    "# 3. Load and extract info from questionnaire responses\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"QUESTIONNAIRE RESPONSES (PANAS, STAI, DIM, SSSQ)\")\n",
    "print(\"=\" * 60)\n",
    "df_quest = pd.read_csv(os.path.join(merged_dir, 'merged_questionnaire_responses.csv'))\n",
    "print(f\"Total rows: {len(df_quest)}\")\n",
    "print(f\"Unique subjects: {df_quest['subject'].nunique()}\")\n",
    "print(f\"Columns: {list(df_quest.columns)}\")\n",
    "print(\"\\nResponses per subject:\")\n",
    "print(df_quest.groupby('subject').size())\n",
    "print(\"\\nDescriptive stats (numeric cols):\")\n",
    "numeric_cols = df_quest.select_dtypes(include=[np.number]).columns\n",
    "print(df_quest[numeric_cols].describe().round(2))\n",
    "\n",
    "# 4. Load and extract info from sensor CSVs (sample for large files)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SENSOR DATA SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "sensor_files = [f for f in csv_files if f.startswith('merged_') and f != 'merged_event_timings.csv' and f != 'merged_questionnaire_responses.csv']\n",
    "for f in sensor_files:\n",
    "    path = os.path.join(merged_dir, f)\n",
    "    size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "    # Use nrows for large files to avoid memory issues\n",
    "    nrows = 100000 if size_mb > 50 else None\n",
    "    df_sensor = pd.read_csv(path, nrows=nrows)\n",
    "    rows_info = f\"{len(df_sensor)} rows (sample)\" if nrows else f\"{len(df_sensor)} rows\"\n",
    "    print(f\"\\n{f}:\")\n",
    "    print(f\"  Shape: {df_sensor.shape}, {rows_info}\")\n",
    "    print(f\"  Columns: {list(df_sensor.columns)}\")\n",
    "    if 'subject' in df_sensor.columns:\n",
    "        print(f\"  Subjects: {df_sensor['subject'].unique().tolist()}\")\n",
    "    sensor_cols = [c for c in df_sensor.columns if c != 'subject']\n",
    "    if sensor_cols:\n",
    "        print(f\"  Sample stats:\\n{df_sensor[sensor_cols].describe().round(4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA for merged datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "merged_dir = '/Users/mostafazamaniturk/Documents/USD/AAI-530/work/WESAD_data/merged_cleaned_data'\n",
    "df_events = pd.read_csv(os.path.join(merged_dir, 'merged_event_timings.csv'))\n",
    "df_quest = pd.read_csv(os.path.join(merged_dir, 'merged_questionnaire_responses.csv'))\n",
    "df_events['Duration'] = df_events['End_Time'] - df_events['Start_Time']\n",
    "\n",
    "# --- 1. EVENT TIMINGS EDA ---\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Event duration distribution by event type\n",
    "ax1 = axes[0]\n",
    "event_order = ['Base', 'TSST', 'Medi 1', 'Fun', 'Medi 2', 'sRead', 'fRead', 'bRead']\n",
    "df_plot = df_events[df_events['Event'].isin(event_order)]\n",
    "sns.boxplot(data=df_plot, x='Event', y='Duration', order=event_order, ax=ax1, palette='viridis')\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "ax1.set_ylabel('Duration (minutes)')\n",
    "ax1.set_title('Event Duration by Event Type')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Events per subject\n",
    "ax2 = axes[1]\n",
    "events_per_subject = df_events.groupby('subject')['Event'].count()\n",
    "events_per_subject.plot(kind='bar', ax=ax2, color='steelblue', edgecolor='black', alpha=0.8)\n",
    "ax2.set_xlabel('Subject')\n",
    "ax2.set_ylabel('Number of Events')\n",
    "ax2.set_title('Events Recorded per Subject')\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. QUESTIONNAIRE EDA ---\n",
    "numeric_cols = df_quest.select_dtypes(include=[np.number]).columns\n",
    "quest_numeric = df_quest[numeric_cols].drop(columns=[c for c in numeric_cols if 'SSSQ' in c], errors='ignore')\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# PANAS summary (first 10 items)\n",
    "panas_cols = [c for c in quest_numeric.columns if c.startswith('PANAS_')][:10]\n",
    "if panas_cols:\n",
    "    panas_means = quest_numeric[panas_cols].mean()\n",
    "    axes[0,0].barh(range(len(panas_means)), panas_means.values, color='coral', alpha=0.8)\n",
    "    axes[0,0].set_yticks(range(len(panas_means)))\n",
    "    axes[0,0].set_yticklabels(panas_cols, fontsize=8)\n",
    "    axes[0,0].set_xlabel('Mean Score')\n",
    "    axes[0,0].set_title('PANAS Mean Scores (first 10 items)')\n",
    "    axes[0,0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# STAI summary\n",
    "stai_cols = [c for c in quest_numeric.columns if c.startswith('STAI_')]\n",
    "if stai_cols:\n",
    "    stai_means = quest_numeric[stai_cols].mean()\n",
    "    axes[0,1].bar(range(len(stai_means)), stai_means.values, color='teal', alpha=0.8)\n",
    "    axes[0,1].set_xticks(range(len(stai_means)))\n",
    "    axes[0,1].set_xticklabels(stai_cols, rotation=45, ha='right')\n",
    "    axes[0,1].set_ylabel('Mean Score')\n",
    "    axes[0,1].set_title('STAI Mean Scores')\n",
    "    axes[0,1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# DIM summary\n",
    "dim_cols = [c for c in quest_numeric.columns if c.startswith('DIM_')]\n",
    "if dim_cols:\n",
    "    axes[1,0].boxplot([quest_numeric[c].dropna() for c in dim_cols], labels=dim_cols)\n",
    "    axes[1,0].set_xticklabels(dim_cols, rotation=45, ha='right')\n",
    "    axes[1,0].set_ylabel('Score')\n",
    "    axes[1,0].set_title('DIM Score Distribution')\n",
    "    axes[1,0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Correlation heatmap (sample of questionnaire scales)\n",
    "scale_cols = panas_cols[:5] + stai_cols + dim_cols if panas_cols else stai_cols + dim_cols\n",
    "corr_cols = [c for c in scale_cols if c in quest_numeric.columns]\n",
    "if len(corr_cols) >= 2:\n",
    "    corr = quest_numeric[corr_cols].corr()\n",
    "    sns.heatmap(corr, annot=True, fmt='.2f', cmap='RdBu_r', center=0, ax=axes[1,1], \n",
    "                xticklabels=True, yticklabels=True, annot_kws={'size': 7})\n",
    "    axes[1,1].set_title('Questionnaire Scale Correlations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. SENSOR DATA EDA (with sampling for large files) ---\n",
    "sensor_files = [f for f in os.listdir(merged_dir) if f.startswith('merged_') \n",
    "                and f not in ['merged_event_timings.csv', 'merged_questionnaire_responses.csv']]\n",
    "\n",
    "# Load sensors with sampling for files > 50 MB\n",
    "sensor_dfs = {}\n",
    "for f in sensor_files:\n",
    "    path = os.path.join(merged_dir, f)\n",
    "    size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "    nrows = 50000 if size_mb > 50 else None  # Sample for large files\n",
    "    sensor_dfs[f] = pd.read_csv(path, nrows=nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensor distributions - univariate signals (EDA, BVP, ECG, EMG, Resp, Temp)\n",
    "univariate_sensors = ['merged_chest_eda.csv', 'merged_wrist_eda.csv', 'merged_wrist_bvp.csv', \n",
    "                      'merged_chest_ecg.csv', 'merged_chest_emg.csv', 'merged_chest_resp.csv',\n",
    "                      'merged_chest_temp.csv', 'merged_wrist_temp.csv']\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, fname in enumerate(univariate_sensors[:8]):\n",
    "    if fname not in sensor_dfs:\n",
    "        continue\n",
    "    df = sensor_dfs[fname]\n",
    "    sensor_col = [c for c in df.columns if c != 'subject'][0]\n",
    "    ax = axes[i]\n",
    "    df[sensor_col].hist(bins=50, ax=ax, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    ax.set_title(f'{fname.replace(\"merged_\", \"\").replace(\".csv\", \"\")}\\n({sensor_col})')\n",
    "    ax.set_xlabel(sensor_col)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Sensor Value Distributions', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject-level sensor summaries (mean values per subject)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Chest EDA by subject\n",
    "df_eda = sensor_dfs.get('merged_chest_eda.csv')\n",
    "if df_eda is not None and 'subject' in df_eda.columns:\n",
    "    eda_by_subj = df_eda.groupby('subject')['EDA'].agg(['mean', 'std'])\n",
    "    eda_by_subj['mean'].plot(kind='bar', ax=axes[0,0], color='darkgreen', alpha=0.8)\n",
    "    axes[0,0].set_title('Mean Chest EDA by Subject')\n",
    "    axes[0,0].set_ylabel('EDA (µS)')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    axes[0,0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Wrist BVP by subject\n",
    "df_bvp = sensor_dfs.get('merged_wrist_bvp.csv')\n",
    "if df_bvp is not None and 'subject' in df_bvp.columns:\n",
    "    bvp_by_subj = df_bvp.groupby('subject')['BVP'].mean()\n",
    "    bvp_by_subj.plot(kind='bar', ax=axes[0,1], color='purple', alpha=0.8)\n",
    "    axes[0,1].set_title('Mean Wrist BVP by Subject')\n",
    "    axes[0,1].set_ylabel('BVP')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    axes[0,1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Chest vs Wrist EDA comparison\n",
    "df_wrist_eda = sensor_dfs.get('merged_wrist_eda.csv')\n",
    "if df_eda is not None and df_wrist_eda is not None:\n",
    "    chest_eda = df_eda.groupby('subject')['EDA'].mean()\n",
    "    wrist_eda = df_wrist_eda.groupby('subject')['EDA'].mean()\n",
    "    subjs = chest_eda.index.intersection(wrist_eda.index)\n",
    "    x = np.arange(len(subjs))\n",
    "    w = 0.35\n",
    "    axes[1,0].bar(x - w/2, chest_eda.reindex(subjs).values, w, label='Chest EDA', color='darkgreen', alpha=0.8)\n",
    "    axes[1,0].bar(x + w/2, wrist_eda.reindex(subjs).values, w, label='Wrist EDA', color='darkorange', alpha=0.8)\n",
    "    axes[1,0].set_xticks(x)\n",
    "    axes[1,0].set_xticklabels(subjs, rotation=45, ha='right')\n",
    "    axes[1,0].set_ylabel('Mean EDA')\n",
    "    axes[1,0].set_title('Chest vs Wrist EDA by Subject')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Accelerometer (wrist) - magnitude distribution\n",
    "df_wrist_acc = sensor_dfs.get('merged_wrist_acc.csv')\n",
    "if df_wrist_acc is not None:\n",
    "    acc_cols = ['ACC_x', 'ACC_y', 'ACC_z']\n",
    "    if all(c in df_wrist_acc.columns for c in acc_cols):\n",
    "        df_wrist_acc = df_wrist_acc.copy()\n",
    "        df_wrist_acc['ACC_mag'] = np.sqrt(df_wrist_acc['ACC_x']**2 + df_wrist_acc['ACC_y']**2 + df_wrist_acc['ACC_z']**2)\n",
    "        df_wrist_acc['ACC_mag'].hist(bins=60, ax=axes[1,1], color='navy', alpha=0.7, edgecolor='black')\n",
    "        axes[1,1].set_title('Wrist Accelerometer Magnitude')\n",
    "        axes[1,1].set_xlabel('Magnitude')\n",
    "        axes[1,1].set_ylabel('Count')\n",
    "        axes[1,1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. DATA QUALITY & MISSING VALUES ---\n",
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUES & DATA QUALITY SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Event timings\n",
    "print(\"\\n1. Event Timings:\")\n",
    "print(f\"   Missing Start_Time: {df_events['Start_Time'].isna().sum()}\")\n",
    "print(f\"   Missing End_Time: {df_events['End_Time'].isna().sum()}\")\n",
    "\n",
    "# Questionnaire\n",
    "print(\"\\n2. Questionnaire Responses:\")\n",
    "print(df_quest.isnull().sum().to_string())\n",
    "\n",
    "# Sensor data (sample)\n",
    "print(\"\\n3. Sensor Data - Missing Values (per sensor):\")\n",
    "for fname, df in sensor_dfs.items():\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.any():\n",
    "        print(f\"   {fname}: {missing.to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. EDA KEY FINDINGS ---\n",
    "print(\"=\" * 60)\n",
    "print(\"EDA KEY FINDINGS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "1. EVENT TIMINGS\n",
    "   - 15 subjects, 8 event types (Base, TSST, Medi 1, Medi 2, Fun, sRead, fRead, bRead)\n",
    "   - Base: ~20 min, TSST: ~11 min, Medi/Fun: ~6-7 min, Read tasks: ~1.2 min\n",
    "   - All subjects have consistent event structure\n",
    "\n",
    "2. QUESTIONNAIRE\n",
    "   - 75 rows (5 responses per subject for PANAS/STAI/DIM across conditions)\n",
    "   - SSSQ has only 15 non-null rows (1 per subject)\n",
    "   - Scales show expected ranges (1-5 for PANAS/STAI, 1-9 for DIM)\n",
    "\n",
    "3. SENSOR DATA\n",
    "   - Chest ACC: very large (~55M rows) - use sampling for analysis\n",
    "   - Chest vs Wrist EDA: different scales (chest typically higher)\n",
    "   - Chest Temp: possible outliers (min -273.15 suggests sensor errors)\n",
    "   - Wrist ACC: integer values, typical range -128 to 127\n",
    "\n",
    "4. DATA QUALITY\n",
    "   - Check chest Temp for sensor malfunction (min -273.15°C)\n",
    "   - SSSQ mostly missing except one response per subject\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Information from Merged CSVs\n",
    "\n",
    "Extract structured metadata and summary statistics from each merged CSV file for downstream analysis and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "merged_dir = '/Users/mostafazamaniturk/Documents/USD/AAI-530/work/WESAD_data/merged_cleaned_data'\n",
    "csv_files = sorted([f for f in os.listdir(merged_dir) if f.endswith('.csv')])\n",
    "LARGE_FILE_THRESHOLD = 1_000_000  # Use sampling for files > 1M rows\n",
    "\n",
    "extraction_results = {}\n",
    "\n",
    "for fname in csv_files:\n",
    "    fpath = os.path.join(merged_dir, fname)\n",
    "    info = {'file': fname, 'path': fpath}\n",
    "    \n",
    "    # Load data (full or sampled for large files)\n",
    "    if os.path.getsize(fpath) > 100_000_000:  # >100MB: use sample\n",
    "        df = pd.read_csv(fpath, nrows=100_000)\n",
    "        n_rows = len(df)  # Approximate; full count would require iterating\n",
    "        info['n_rows'] = f\">100k (file ~{os.path.getsize(fpath)/1e6:.0f} MB)\"\n",
    "        info['sampled'] = True\n",
    "    else:\n",
    "        df = pd.read_csv(fpath)\n",
    "        n_rows = len(df)\n",
    "        info['n_rows'] = n_rows\n",
    "        info['sampled'] = n_rows > LARGE_FILE_THRESHOLD\n",
    "    \n",
    "    info['columns'] = list(df.columns)\n",
    "    info['dtypes'] = {c: str(d) for c, d in df.dtypes.items()}\n",
    "    \n",
    "    # Numeric columns for statistics\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = [c for c in df.columns if c not in num_cols]\n",
    "    \n",
    "    if num_cols:\n",
    "        stats = df[num_cols].describe().to_dict()\n",
    "        info['stats'] = {col: {k: float(v) if np.isfinite(v) else None \n",
    "                               for k, v in s.items()} for col, s in stats.items()}\n",
    "    \n",
    "    if 'subject' in df.columns:\n",
    "        info['subjects'] = sorted(df['subject'].unique().tolist())\n",
    "        info['n_subjects'] = len(info['subjects'])\n",
    "        # Per-subject row counts\n",
    "        info['rows_per_subject'] = df.groupby('subject').size().to_dict()\n",
    "    \n",
    "    # Event timings: extract event info\n",
    "    if fname == 'merged_event_timings.csv':\n",
    "        info['events'] = df['Event'].unique().tolist() if 'Event' in df.columns else []\n",
    "        info['event_durations'] = (df['End_Time'] - df['Start_Time']).describe().to_dict() if 'End_Time' in df.columns else {}\n",
    "    \n",
    "    # Questionnaire: extract scale summaries\n",
    "    if fname == 'merged_questionnaire_responses.csv':\n",
    "        panas_cols = [c for c in df.columns if c.startswith('PANAS_')]\n",
    "        stai_cols = [c for c in df.columns if c.startswith('STAI_')]\n",
    "        if panas_cols:\n",
    "            info['panas_summary'] = df[panas_cols].describe().iloc[1:4].to_dict()\n",
    "        if stai_cols:\n",
    "            info['stai_summary'] = df[stai_cols].describe().iloc[1:4].to_dict()\n",
    "    \n",
    "    # Missing values\n",
    "    info['missing_counts'] = df.isnull().sum().to_dict()\n",
    "    info['missing_pct'] = (df.isnull().sum() / len(df) * 100).round(2).to_dict()\n",
    "    \n",
    "    extraction_results[fname] = info\n",
    "    print(f\"Extracted: {fname} ({n_rows:,} rows, {len(df.columns)} cols)\")\n",
    "\n",
    "# Summary DataFrame\n",
    "summary_rows = []\n",
    "for fname, info in extraction_results.items():\n",
    "    summary_rows.append({\n",
    "        'file': fname,\n",
    "        'n_rows': info['n_rows'],\n",
    "        'n_cols': len(info['columns']),\n",
    "        'n_subjects': info.get('n_subjects', ''),\n",
    "        'sampled': info.get('sampled', False),\n",
    "    })\n",
    "df_extraction_summary = pd.DataFrame(summary_rows)\n",
    "print(\"\\n--- EXTRACTION SUMMARY ---\")\n",
    "print(df_extraction_summary.to_string(index=False))\n",
    "\n",
    "# Per-file extracted info available in extraction_results dict\n",
    "# Example: extraction_results['merged_event_timings.csv']['events']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display extracted information by category\n",
    "\n",
    "# 1. Event timings\n",
    "ev = extraction_results.get('merged_event_timings.csv', {})\n",
    "if ev:\n",
    "    print(\"EVENT TIMINGS:\")\n",
    "    print(f\"  Events: {ev.get('events', [])}\")\n",
    "    print(f\"  Subjects: {ev.get('n_subjects')}\")\n",
    "    if ev.get('event_durations'):\n",
    "        dur = ev['event_durations']\n",
    "        mn, me, mx = dur.get('min', 0), dur.get('mean', 0), dur.get('max', 0)\n",
    "        print(f\"  Duration (min): {float(mn):.2f}s, mean: {float(me):.2f}s, max: {float(mx):.2f}s\")\n",
    "\n",
    "# 2. Questionnaire\n",
    "quest = extraction_results.get('merged_questionnaire_responses.csv', {})\n",
    "if quest:\n",
    "    print(\"\\nQUESTIONNAIRE:\")\n",
    "    print(f\"  Subjects: {quest.get('n_subjects')}\")\n",
    "    print(f\"  Scale columns: PANAS (26), STAI (6), DIM (2), SSSQ (6)\")\n",
    "\n",
    "# 3. Sensor files - per-subject row counts\n",
    "print(\"\\nSENSOR DATA - ROWS PER SUBJECT (sample files):\")\n",
    "for fname in ['merged_wrist_eda.csv', 'merged_chest_eda.csv', 'merged_wrist_temp.csv']:\n",
    "    info = extraction_results.get(fname, {})\n",
    "    if info and 'rows_per_subject' in info:\n",
    "        rps = info['rows_per_subject']\n",
    "        print(f\"  {fname}: min={min(rps.values()):,}, max={max(rps.values()):,}\")\n",
    "\n",
    "# 4. Stats sample for one sensor\n",
    "print(\"\\nSAMPLE STATS (merged_wrist_bvp.csv):\")\n",
    "bvp = extraction_results.get('merged_wrist_bvp.csv', {})\n",
    "if bvp and 'stats' in bvp and 'BVP' in bvp.get('stats', {}):\n",
    "    for k, v in bvp['stats']['BVP'].items():\n",
    "        if v is not None:\n",
    "            print(f\"  BVP {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Listing contents of MyDrive:')\n",
    "! ls \"/Users/mostafazamaniturk/Documents/USD/AAI-530/work/WESAD_data/merged_cleaned_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dir = '/Users/mostafazamaniturk/Documents/USD/AAI-530/work/WESAD_data/merged_cleaned_data'\n",
    "bvp_dir = '/Users/mostafazamaniturk/Documents/USD/AAI-530/work/WESAD_data/merged_cleaned_data/merged_wrist_bvp.csv'\n",
    "eda_dir = '/Users/mostafazamaniturk/Documents/USD/AAI-530/work/WESAD_data/merged_cleaned_data/merged_wrist_eda.csv'\n",
    "event_timing_dir = '/Users/mostafazamaniturk/Documents/USD/AAI-530/work/WESAD_data/merged_cleaned_data/merged_event_timings.csv'\n",
    "\n",
    "df_bvp = pd.read_csv(bvp_dir)\n",
    "df_eda = pd.read_csv(eda_dir)\n",
    "df_event_timing = pd.read_csv(event_timing_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bvp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bvp.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bvp.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_timing.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION (CHECK THESE!)\n",
    "# ==========================================\n",
    "FILE_BVP = bvp_dir\n",
    "FILE_EDA = eda_dir\n",
    "OUTPUT_FILE = 'aligned_wrist_data_64Hz.csv'\n",
    "\n",
    "# Sampling Rates (Fixed by Empatica E4 hardware)\n",
    "FS_BVP = 64.0  # Hz\n",
    "FS_EDA = 4.0   # Hz\n",
    "\n",
    "# ==========================================\n",
    "# 2. LOAD DATA & DETECT COLUMNS\n",
    "# ==========================================\n",
    "print(\"--- Loading Data Previews ---\")\n",
    "# Load just the first row to check headers\n",
    "bvp_preview = pd.read_csv(FILE_BVP, nrows=1)\n",
    "eda_preview = pd.read_csv(FILE_EDA, nrows=1)\n",
    "\n",
    "print(f\"BVP Columns found: {list(bvp_preview.columns)}\")\n",
    "print(f\"EDA Columns found: {list(eda_preview.columns)}\")\n",
    "\n",
    "# *** UPDATE THESE STRINGS BASED ON THE PRINT OUTPUT ABOVE ***\n",
    "# Example: If your column is named 'subject_id', change 'id' to 'subject_id'\n",
    "COL_SUBJECT = 'subject'   # The column identifying the person (e.g. S2, S3...)\n",
    "COL_BVP_VAL = 'BVP'       # The column with Blood Volume Pulse values\n",
    "COL_EDA_VAL = 'EDA'       # The column with Skin Conductance values\n",
    "\n",
    "# Verify columns exist before proceeding\n",
    "if COL_SUBJECT not in bvp_preview.columns:\n",
    "    print(f\"\\n[ERROR] Column '{COL_SUBJECT}' not found in BVP file.\")\n",
    "    print(f\"Please look at the 'BVP Columns found' above and update COL_SUBJECT in the script.\")\n",
    "    sys.exit()\n",
    "\n",
    "# ==========================================\n",
    "# 3. ALIGNMENT FUNCTION\n",
    "# ==========================================\n",
    "def align_subject(subject_id, df_bvp_sub, df_eda_sub):\n",
    "    \"\"\"\n",
    "    Aligns low-freq EDA (4Hz) to high-freq BVP (64Hz) for a single subject.\n",
    "    \"\"\"\n",
    "    # 1. Generate Relative Time Indices (Seconds since start)\n",
    "    # -----------------------------------------------------\n",
    "    # BVP Time: 0, 0.015625, 0.03125...\n",
    "    n_bvp = len(df_bvp_sub)\n",
    "    time_bvp = np.arange(n_bvp) / FS_BVP\n",
    "    \n",
    "    # EDA Time: 0, 0.25, 0.50...\n",
    "    n_eda = len(df_eda_sub)\n",
    "    time_eda = np.arange(n_eda) / FS_EDA\n",
    "\n",
    "    # 2. Create Series with Time as Index\n",
    "    # -----------------------------------------------------\n",
    "    bvp_series = pd.Series(df_bvp_sub[COL_BVP_VAL].values, index=time_bvp)\n",
    "    eda_series = pd.Series(df_eda_sub[COL_EDA_VAL].values, index=time_eda)\n",
    "\n",
    "    # 3. Upsample EDA to match BVP Time\n",
    "    # -----------------------------------------------------\n",
    "    # Reindex creates NaNs at the new 64Hz timestamps\n",
    "    # Interpolate fills them linearly\n",
    "    # ffill/bfill handles the very first/last fractional seconds\n",
    "    eda_aligned = eda_series.reindex(bvp_series.index).interpolate(method='linear').ffill().bfill()\n",
    "\n",
    "    # 4. Combine into a clean DataFrame\n",
    "    # -----------------------------------------------------\n",
    "    df_aligned = pd.DataFrame({\n",
    "        COL_SUBJECT: subject_id,\n",
    "        'time_sec': bvp_series.index,  # Relative time (0.00 to end)\n",
    "        'bvp': bvp_series.values,\n",
    "        'eda': eda_aligned.values\n",
    "    })\n",
    "    \n",
    "    return df_aligned\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN PROCESSING LOOP\n",
    "# ==========================================\n",
    "print(\"\\n--- Loading Full Datasets ---\")\n",
    "df_bvp_all = pd.read_csv(FILE_BVP)\n",
    "df_eda_all = pd.read_csv(FILE_EDA)\n",
    "\n",
    "print(f\"BVP Total Rows: {len(df_bvp_all)}\")\n",
    "print(f\"EDA Total Rows: {len(df_eda_all)}\")\n",
    "\n",
    "aligned_dfs = []\n",
    "unique_subjects = df_bvp_all[COL_SUBJECT].unique()\n",
    "print(f\"\\nProcessing {len(unique_subjects)} subjects: {unique_subjects}\")\n",
    "\n",
    "for sub_id in unique_subjects:\n",
    "    # Extract this subject's data\n",
    "    sub_bvp = df_bvp_all[df_bvp_all[COL_SUBJECT] == sub_id]\n",
    "    sub_eda = df_eda_all[df_eda_all[COL_SUBJECT] == sub_id]\n",
    "\n",
    "    # Safety check: Does this subject exist in both files?\n",
    "    if sub_eda.empty:\n",
    "        print(f\"  [WARN] Subject {sub_id} found in BVP but MISSING in EDA. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Run Alignment\n",
    "    try:\n",
    "        aligned_chunk = align_subject(sub_id, sub_bvp, sub_eda)\n",
    "        aligned_dfs.append(aligned_chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"  [ERROR] Failed to align Subject {sub_id}: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. SAVE RESULTS\n",
    "# ==========================================\n",
    "if aligned_dfs:\n",
    "    print(\"\\n--- Merging & Saving ---\")\n",
    "    final_df = pd.concat(aligned_dfs, ignore_index=True)\n",
    "    \n",
    "    final_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"Success! Aligned data saved to: {OUTPUT_FILE}\")\n",
    "    print(f\"Final Shape: {final_df.shape}\")\n",
    "    print(final_df.head())\n",
    "else:\n",
    "    print(\"\\n[ERROR] No data was aligned. Check your inputs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels from merged_event_timings to aligned_wrist_data_64Hz.csv\n",
    "# Uses merged_event_timings.csv - event Start_Time/End_Time are in MINUTES (from quest); convert to seconds.\n",
    "# Label: 0=Baseline, 1=Stress(TSST), 2=Amusement(Fun), -1=Other/Transition(Medi,Read)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "ALIGNED_FILE = '/Users/mostafazamaniturk/Documents/USD/AAI-530/work/edaFolder/aligned_wrist_data_64Hz.csv'\n",
    "EVENT_TIMINGS_FILE = '/Users/mostafazamaniturk/Documents/USD/AAI-530/work/WESAD_data/merged_cleaned_data/merged_event_timings.csv'\n",
    "\n",
    "EVENT_TO_LABEL = {'Base': 0, 'TSST': 1, 'Fun': 2}  # Medi, Read, etc -> -1\n",
    "\n",
    "df = pd.read_csv(ALIGNED_FILE)\n",
    "df_events = pd.read_csv(EVENT_TIMINGS_FILE)\n",
    "df_events['label'] = df_events['Event'].map(lambda e: EVENT_TO_LABEL.get(e, -1))\n",
    "\n",
    "# Event times in merged_event_timings are in MINUTES from session start; time_sec is in seconds.\n",
    "labels = np.full(len(df), -1, dtype=np.int64)\n",
    "for subj in df['subject'].unique():\n",
    "    mask = (df['subject'] == subj).values\n",
    "    idx = np.where(mask)[0]\n",
    "    times = df.loc[mask, 'time_sec'].values\n",
    "    subj_events = df_events[df_events['subject'] == subj]\n",
    "    for _, ev in subj_events.iterrows():\n",
    "        start_sec = ev['Start_Time'] * 60.0\n",
    "        end_sec = ev['End_Time'] * 60.0\n",
    "        in_range = (times >= start_sec) & (times <= end_sec)\n",
    "        labels[idx] = np.where(in_range, ev['label'], labels[idx])\n",
    "\n",
    "df['label'] = labels\n",
    "\n",
    "# Save labeled dataset\n",
    "df.to_csv(ALIGNED_FILE, index=False)\n",
    "print(f'Done! Labels added. Saved to {ALIGNED_FILE}')\n",
    "print(f'Label distribution:\\n{df[\"label\"].value_counts().sort_index()}')\n",
    "print(f'\\nSample (first 10 rows):')\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Listing contents of MyDrive:')\n",
    "! ls \"/Users/mostafazamaniturk/Documents/USD/AAI-530/work/edaFolder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_dir = '/Users/mostafazamaniturk/Documents/USD/AAI-530/work/edaFolder/aligned_wrist_data_64Hz.csv'\n",
    "\n",
    "df = pd.read_csv(aligned_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify labeled data (run after the label-assignment cell above)\n",
    "ALIGNED_FILE = '/Users/mostafazamaniturk/Documents/USD/AAI-530/work/edaFolder/aligned_wrist_data_64Hz.csv'\n",
    "df = pd.read_csv(ALIGNED_FILE)\n",
    "print('Label distribution (0=baseline, 1=stress, 2=amusement, -1=other):')\n",
    "print(df['label'].value_counts().sort_index())\n",
    "print('\\nColumns:', list(df.columns))\n",
    "print('\\nSample with labels:')\n",
    "print(df.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Full Pipeline from Original Pickle (Guarantees Full Session)\n",
    "\n",
    "If merged CSVs were created with truncated data, run this cell to load directly from pickle files, align wrist BVP+EDA, and assign labels using `merged_event_timings.csv`. Ensures entire session (~1.5 hr/subject) is exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline from pickle: load, align, label. Run if merged CSVs may be truncated.\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "base_path = '/Users/mostafazamaniturk/Documents/USD/AAI-530/work'\n",
    "wesad_base = os.path.join(base_path, 'WESAD_data', 'WESAD')\n",
    "merged_dir = os.path.join(base_path, 'WESAD_data', 'merged_cleaned_data')\n",
    "SUBJECTS = ['S2','S3','S4','S5','S6','S7','S8','S9','S10','S11','S13','S14','S15','S16','S17']\n",
    "FS_BVP, FS_EDA = 64.0, 4.0\n",
    "EVENT_TO_LABEL = {'Base': 0, 'TSST': 1, 'Fun': 2}\n",
    "\n",
    "def _clean_split(s):\n",
    "    return [p.strip() for p in s.replace('#','').split(';') if p.strip()]\n",
    "\n",
    "aligned_dfs = []\n",
    "for subj in SUBJECTS:\n",
    "    pkl_path = os.path.join(wesad_base, subj, f'{subj}.pkl')\n",
    "    quest_path = os.path.join(wesad_base, subj, f'{subj}_quest.csv')\n",
    "    if not os.path.exists(pkl_path) or not os.path.exists(quest_path):\n",
    "        print(f\"Skipping {subj}: missing files\"); continue\n",
    "    raw = pd.read_pickle(pkl_path)\n",
    "    bvp = raw['signal']['wrist']['BVP'].flatten()\n",
    "    eda = raw['signal']['wrist']['EDA'].flatten()\n",
    "    # Align EDA (4Hz) to BVP (64Hz)\n",
    "    time_bvp = np.arange(len(bvp)) / FS_BVP\n",
    "    time_eda = np.arange(len(eda)) / FS_EDA\n",
    "    eda_s = pd.Series(eda, index=time_eda).reindex(time_bvp).interpolate().ffill().bfill()\n",
    "    df = pd.DataFrame({'subject':subj, 'time_sec':time_bvp, 'bvp':bvp, 'eda':eda_s.values})\n",
    "    # Event timings from quest (Start/End in minutes)\n",
    "    q = pd.read_csv(quest_path)\n",
    "    order = _clean_split(q.iloc[0,0])[1:]\n",
    "    parts_s = _clean_split(q.iloc[1,0])\n",
    "    parts_e = _clean_split(q.iloc[2,0])\n",
    "    labels = np.full(len(df), -1, dtype=np.int64)\n",
    "    for i, e in enumerate(order):\n",
    "        if i+1 >= len(parts_s) or i+1 >= len(parts_e): continue\n",
    "        try:\n",
    "            st_min, end_min = float(parts_s[i+1]), float(parts_e[i+1])\n",
    "            lbl = EVENT_TO_LABEL.get(e, -1)\n",
    "            in_range = (df['time_sec'] >= st_min*60) & (df['time_sec'] <= end_min*60)\n",
    "            labels = np.where(in_range, lbl, labels)\n",
    "        except (ValueError, IndexError): pass\n",
    "    df['label'] = labels\n",
    "    aligned_dfs.append(df)\n",
    "    print(f\"  {subj}: {len(df)} rows, {len(df)/FS_BVP/60:.1f} min\")\n",
    "\n",
    "if aligned_dfs:\n",
    "    out = pd.concat(aligned_dfs, ignore_index=True)\n",
    "    out_path = os.path.join(base_path, 'edaFolder', 'aligned_wrist_data_64Hz.csv')\n",
    "    out.to_csv(out_path, index=False)\n",
    "    print(f\"\\nSaved {out_path} | Shape: {out.shape}\")\n",
    "    print(f\"Label counts:\\n{out['label'].value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Exploratory Data Analysis (Aligned Wrist 64Hz)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Path to aligned wrist BVP/EDA data for EDA section\n",
    "DATA_PATH = '/Users/mostafazamaniturk/Documents/USD/AAI-530/work/aligned_wrist_data_64Hz_v1.csv'\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    DATA_PATH = os.path.join(os.getcwd(), 'aligned_wrist_data_64Hz.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data & Basic Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance Check (Stress Prediction)\n",
    "\n",
    "For ML stress prediction: exclude label -1 (other/transition), use labels 0=baseline, 1=stress, 2=amusement.  \n",
    "Stress (1) is the target of interest—check imbalance and get class weights for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = {-1: 'other', 0: 'baseline', 1: 'stress', 2: 'amusement'}\n",
    "\n",
    "# ML-ready subset: exclude -1 (other/transition); use only 0, 1, 2 for stress prediction\n",
    "df_ml = df[df['label'] >= 0].copy()\n",
    "print(f\"ML dataset (labels 0,1,2 only): {len(df_ml):,} samples\\n\")\n",
    "\n",
    "# --- 3-class: baseline, stress, amusement ---\n",
    "counts_3 = df_ml['label'].value_counts().sort_index()\n",
    "total_3 = len(df_ml)\n",
    "stress_count = counts_3.get(1, 0)\n",
    "stress_pct = 100 * stress_count / total_3\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"STRESS PREDICTION — CLASS IMBALANCE CHECK\")\n",
    "print(\"=\" * 55)\n",
    "print(\"\\n--- 3-Class (baseline, stress, amusement) ---\")\n",
    "for lbl in [0, 1, 2]:\n",
    "    cnt = counts_3.get(lbl, 0)\n",
    "    pct = 100 * cnt / total_3\n",
    "    marker = \" ★ TARGET\" if lbl == 1 else \"\"\n",
    "    print(f\"  {label_names[lbl]} ({lbl}): {cnt:,} samples ({pct:.2f}%){marker}\")\n",
    "\n",
    "majority_3 = counts_3.idxmax()\n",
    "minority_3 = counts_3.idxmin()\n",
    "ratio_3 = counts_3.max() / counts_3.min()\n",
    "print(f\"\\n  Imbalance ratio: {ratio_3:.2f}  |  Stress samples: {stress_count:,}\")\n",
    "\n",
    "# --- Binary: stress (1) vs non-stress (0+2) ---\n",
    "df_binary = df_ml.copy()\n",
    "df_binary['stress_binary'] = (df_binary['label'] == 1).astype(int)\n",
    "counts_bin = df_binary['stress_binary'].value_counts()\n",
    "stress_bin = counts_bin.get(1, 0)\n",
    "non_stress_bin = counts_bin.get(0, 0)\n",
    "ratio_bin = max(stress_bin, non_stress_bin) / min(stress_bin, non_stress_bin)\n",
    "\n",
    "print(\"\\n--- Binary (stress vs non-stress) ---\")\n",
    "print(f\"  stress (1):    {stress_bin:,} samples ({100*stress_bin/total_3:.2f}%)\")\n",
    "print(f\"  non-stress:    {non_stress_bin:,} samples ({100*non_stress_bin/total_3:.2f}%)\")\n",
    "print(f\"  Imbalance ratio: {ratio_bin:.2f}\")\n",
    "\n",
    "# --- Class weights for sklearn (3-class) ---\n",
    "try:\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    y_3 = df_ml['label'].values\n",
    "    weights_3 = compute_class_weight('balanced', classes=np.array([0, 1, 2]), y=y_3)\n",
    "    class_weights_3 = {i: w for i, w in zip([0, 1, 2], weights_3)}\n",
    "except ImportError:\n",
    "    # Fallback: balanced weight = n_samples / (n_classes * n_samples_per_class)\n",
    "    n = len(df_ml)\n",
    "    class_weights_3 = {lbl: n / (3 * cnt) for lbl, cnt in counts_3.items()}\n",
    "print(\"\\n--- Class weights (for 3-class ML) ---\")\n",
    "for lbl in sorted(class_weights_3):\n",
    "    print(f\"  {label_names[lbl]} ({lbl}): {class_weights_3[lbl]:.4f}\")\n",
    "print(\"  Use in model: class_weight=class_weights_3\")\n",
    "\n",
    "# --- Verdict & recommendations ---\n",
    "print(\"\\n--- Verdict ---\")\n",
    "if ratio_3 > 1.5:\n",
    "    print(f\"⚠️  Dataset is IMBALANCED (ratio {ratio_3:.2f} > 1.5)\")\n",
    "    print(\"\\n  ML recommendations for stress prediction:\")\n",
    "    print(\"  • Use class_weight='balanced' or class_weights_3 above\")\n",
    "    print(\"  • Consider SMOTE/oversampling on stress class to boost recall\")\n",
    "    print(\"  • Use recall/F1 for stress class—avoid missing stressed users\")\n",
    "    print(\"  • Stratify train/test splits: stratify=df_ml['label']\")\n",
    "else:\n",
    "    print(f\"✓ Dataset is relatively balanced (ratio {ratio_3:.2f})\")\n",
    "\n",
    "# Keep df_ml for model training\n",
    "print(f\"\\ndf_ml ready: {len(df_ml):,} samples for stress prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape:', df.shape)\n",
    "print('\\nColumns:', df.columns.tolist())\n",
    "print('\\nData types:')\n",
    "print(df.dtypes)\n",
    "print('\\nMemory usage:', df.memory_usage(deep=True).sum() / 1024**2, 'MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Label Distribution\n",
    "\n",
    "WESAD labels: **0**=baseline, **1**=stress, **2**=amusement, **-1**=other/transition (Medi, Read events)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution\n",
    "label_counts = df['label'].value_counts().sort_index()\n",
    "label_names = {-1: 'other', 0: 'baseline', 1: 'stress', 2: 'amusement'}\n",
    "print('Label distribution:')\n",
    "for k, v in label_counts.items():\n",
    "    print(f'  {label_names.get(k, k)} ({k}): {v:,} ({100*v/len(df):.1f}%)')\n",
    "print(f'\\nThree-class (0,1,2) samples: {(df[\"label\"] >= 0).sum():,}')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "labels_sorted = sorted(label_counts.index)\n",
    "colors = ['gray', 'steelblue', 'coral', 'seagreen']\n",
    "ax.bar([label_names.get(l, str(l)) for l in labels_sorted], \n",
    "       [label_counts[l] for l in labels_sorted], \n",
    "       color=[colors[i] for i in range(len(labels_sorted))], edgecolor='black')\n",
    "ax.set_xlabel('Label')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Distribution of Stress/Affect Labels')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BVP and EDA by Label\n",
    "\n",
    "Compare signal statistics across stress conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to three-class labels (exclude -1) for cleaner comparison\n",
    "df_labels = df[df['label'] >= 0].copy()\n",
    "df_labels['label_name'] = df_labels['label'].map(label_names)\n",
    "\n",
    "# Per-label statistics\n",
    "agg_label = df_labels.groupby('label').agg({'bvp': ['mean', 'std'], 'eda': ['mean', 'std']})\n",
    "print('BVP and EDA statistics by label:')\n",
    "print(agg_label.round(4))\n",
    "\n",
    "# Boxplots by label (order: baseline, stress, amusement)\n",
    "order = ['baseline', 'stress', 'amusement']\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "for ax, col in zip(axes, ['bvp', 'eda']):\n",
    "    data_by_label = [df_labels[df_labels['label'] == k][col] for k in [0, 1, 2]]\n",
    "    bp = ax.boxplot(data_by_label, labels=order, patch_artist=True)\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set_facecolor('lightsteelblue')\n",
    "    ax.set_title(f'{col.upper()} by Label')\n",
    "    ax.set_ylabel(col.upper())\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "pd.DataFrame({'Missing Count': missing, 'Missing %': missing_pct})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Subject Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_counts = df['subject'].value_counts().sort_index()\n",
    "print('Number of unique subjects:', df['subject'].nunique())\n",
    "print('\\nRows per subject:')\n",
    "print(subject_counts)\n",
    "print('\\nDuration per subject (approx minutes, at 64 Hz):')\n",
    "for subj in subject_counts.index:\n",
    "    n = subject_counts[subj]\n",
    "    dur_min = n / 64 / 60\n",
    "    print(f'  {subj}: {dur_min:.1f} min ({n:,} samples)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "subject_counts.plot(kind='bar', ax=ax, color='steelblue', edgecolor='black')\n",
    "ax.set_title('Sample Count per Subject')\n",
    "ax.set_xlabel('Subject')\n",
    "ax.set_ylabel('Number of Samples')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Univariate Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# BVP histogram\n",
    "axes[0, 0].hist(df['bvp'].dropna(), bins=80, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('BVP (Blood Volume Pulse) Distribution')\n",
    "axes[0, 0].set_xlabel('BVP value')\n",
    "\n",
    "# EDA histogram\n",
    "axes[0, 1].hist(df['eda'].dropna(), bins=80, color='seagreen', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_title('EDA (Electrodermal Activity) Distribution')\n",
    "axes[0, 1].set_xlabel('EDA value (µS)')\n",
    "\n",
    "# Time distribution\n",
    "axes[1, 0].hist(df['time_sec'], bins=50, color='slateblue', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_title('Time Distribution (seconds)')\n",
    "axes[1, 0].set_xlabel('time_sec')\n",
    "\n",
    "# Box plots for BVP and EDA\n",
    "df[['bvp', 'eda']].boxplot(ax=axes[1, 1])\n",
    "axes[1, 1].set_title('BVP vs EDA Box Plots')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Time Series Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample one subject for visualization (first 5000 points ~78 sec)\n",
    "sample_subj = df['subject'].iloc[0]\n",
    "sample = df[df['subject'] == sample_subj].head(5000)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 6), sharex=True)\n",
    "axes[0].plot(sample['time_sec'], sample['bvp'], color='coral', linewidth=0.5)\n",
    "axes[0].set_ylabel('BVP')\n",
    "axes[0].set_title(f'BVP Time Series - Subject {sample_subj} (first ~78 sec)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(sample['time_sec'], sample['eda'], color='seagreen', linewidth=0.5)\n",
    "axes[1].set_ylabel('EDA (µS)')\n",
    "axes[1].set_xlabel('Time (seconds)')\n",
    "axes[1].set_title(f'EDA Time Series - Subject {sample_subj}')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Colored by Label\n",
    "\n",
    "BVP and EDA for one subject with segments colored by condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series with label-colored segments (sample ~3 min of one subject)\n",
    "sample_subj = df['subject'].iloc[0]\n",
    "sample = df[df['subject'] == sample_subj].head(12000)  # ~3 min at 64 Hz\n",
    "\n",
    "label_colors = {-1: 'lightgray', 0: 'steelblue', 1: 'coral', 2: 'seagreen'}\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 6), sharex=True)\n",
    "\n",
    "for ax, col, ylab in zip(axes, ['bvp', 'eda'], ['BVP', 'EDA (µS)']):\n",
    "    for lbl in [-1, 0, 1, 2]:\n",
    "        mask = sample['label'] == lbl\n",
    "        if mask.any():\n",
    "            ax.scatter(sample.loc[mask, 'time_sec'], sample.loc[mask, col], \n",
    "                      c=label_colors[lbl], s=1, alpha=0.6, label=label_names.get(lbl, lbl))\n",
    "    ax.set_ylabel(ylab)\n",
    "    ax.legend(loc='upper right', markerscale=5)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "axes[1].set_xlabel('Time (seconds)')\n",
    "axes[0].set_title(f'BVP & EDA by Label - Subject {sample_subj}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df[['bvp', 'eda']].corr()\n",
    "print('Correlation matrix:')\n",
    "print(corr)\n",
    "\n",
    "# Scatter plot (sample for readability)\n",
    "sample_size = min(20000, len(df))\n",
    "sample_df = df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(sample_df['bvp'], sample_df['eda'], alpha=0.3, s=5)\n",
    "ax.set_xlabel('BVP')\n",
    "ax.set_ylabel('EDA (µS)')\n",
    "ax.set_title('BVP vs EDA Scatter (sampled 20k points)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Per-Subject Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = df.groupby('subject').agg({\n",
    "    'bvp': ['mean', 'std', 'min', 'max', 'median'],\n",
    "    'eda': ['mean', 'std', 'min', 'max', 'median']\n",
    "}).round(4)\n",
    "agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "df.boxplot(column='bvp', by='subject', ax=axes[0])\n",
    "axes[0].set_title('BVP by Subject')\n",
    "axes[0].set_xlabel('Subject')\n",
    "\n",
    "df.boxplot(column='eda', by='subject', ax=axes[1])\n",
    "axes[1].set_title('EDA by Subject')\n",
    "axes[1].set_xlabel('Subject')\n",
    "\n",
    "plt.suptitle('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_outliers(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    return (series < lower) | (series > upper)\n",
    "\n",
    "bvp_out = iqr_outliers(df['bvp'])\n",
    "eda_out = iqr_outliers(df['eda'])\n",
    "\n",
    "print('BVP outliers (IQR method):', bvp_out.sum(), f'({bvp_out.sum()/len(df)*100:.2f}%)')\n",
    "print('EDA outliers (IQR method):', eda_out.sum(), f'({eda_out.sum()/len(df)*100:.2f}%)')\n",
    "print('\\nBVP outlier bounds:', df['bvp'].quantile(0.25) - 1.5*(df['bvp'].quantile(0.75)-df['bvp'].quantile(0.25)), 'to',\n",
    "      df['bvp'].quantile(0.75) + 1.5*(df['bvp'].quantile(0.75)-df['bvp'].quantile(0.25)))\n",
    "print('EDA outlier bounds:', df['eda'].quantile(0.25) - 1.5*(df['eda'].quantile(0.75)-df['eda'].quantile(0.25)), 'to',\n",
    "      df['eda'].quantile(0.75) + 1.5*(df['eda'].quantile(0.75)-df['eda'].quantile(0.25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sampling Rate Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify 64 Hz: delta should be ~1/64 sec\n",
    "sample = df[df['subject'] == df['subject'].iloc[0]].head(100)\n",
    "deltas = sample['time_sec'].diff().dropna()\n",
    "print('Expected sampling interval: 1/64 =', 1/64, 'sec')\n",
    "print('Observed mean interval:', deltas.mean(), 'sec')\n",
    "print('Observed std of intervals:', deltas.std(), 'sec')\n",
    "print('Implied sampling rate:', 1/deltas.mean(), 'Hz')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
